## Planning Data Transfer

#### Minimize
Since, CPU <-->GPU memory transfer is expensive, we try to minimize necessary moves and try to eliminate unnecessary moves altogether. 

#### Frequency
Another thing we can do is transfer data less often i.e. Since the overhead of data transfer is persistent on both large chunks and small chunks, instead of sending small chunks many times, we try sending large chunks less often.

#### Pinned Memory
We can also use pinned memory. Pinned memory (**a.k.a Page-locked memory**) is memory in the RAM (host) that we explicitly reserve and lock in place. By doing this the GPU can access it directly without going through the OS's paging mechanism. This speeds up data transfer compared to <i >pageable</i> memory which is the default method of data transfer between host and device that maps virtual memory space to the physical memory pages in the RAM.
###### How to:
To do this in CUDA we use `cudaMallocHost((void**)A, size)` to pin a certain memory. After use we can use `cudaFreeHost(A)` to free that memory.

## Streams 
In CUDA, a **stream** is a sequence of operations that the GPU executes in order. Operations in different streams can run at the same time, but operations in the same stream are done one after another. Tasks from different streams can be mixed together to make better use of the GPU.

###### How does this optimize memory and communication:
Streams allow for asynchronous execution, which means multiple tasks like memory transfer (e.g. host to device) can run at the same time as kernel executions. This improves GPU resource utilization i.e. the GPU doesn't have to sit idly while it waits for the data transfer.

![Screenshot 2025-04-08 225216](https://github.com/user-attachments/assets/09e1b438-0a51-4086-838b-8fc2cb8c9009)

To initiate streams in CUDA we use `cudaStreamCreate()` 

#### CUDA Memory Hierarchy Comparison

| Speed               | Memory Type     | Declaration Example   | Scope  |
| ------------------- | --------------- | --------------------- | ------ |
| Fastest             | Register        | `int v;` (implicit)   | thread |
| Very Fast           | Local Memory    | `int v[10];`          | thread |
| Fast                | Shared Memory   | `__shared__ int v;`   | block  |
| Fast (with caching) | Global Memory   | `__device__ int v;`   | grid   |
| Slowest             | Constant Memory | `__constant__ int v;` | grid   |
###### Key Points:
- **Registers** are the fastest but are limited in number, so excessive use can lead to spilling to local memory (which is slower).
- **Shared memory** is fast and shared between threads of the same block. Use when threads in the SM need to access the same data repeatedly. Faster than global and reduces the need to access global memory. Use in sorting algos, matmul, etc.
- **Constant memory** is cached and fast for read-only data, use especially when many threads need to read to the same data. Size is 64KB so pick constant data efficiently.

**Note: Read stencil computation code to understand how shared memory is used to reduce global memory access latency**


#### An example with Streams and Pinned memory

```c
#include <iostream>
#include <cuda_runtime.h>

__global__ void kernel1(int *data) {
    /* Perform operation on data in kernel1 */
}

__global__ void kernel2(int *data) {
    /* Perform operation on data in kernel2 */
}

int main() {
    int N = 256;
    int size = N * sizeof(int);

    // Memory Initialization Section
    int *d_data;
    int *h_data;
    cudaMallocHost((void**)&h_data, size);  // Pinned memory for h_data
    for (int i = 0; i < N; i++) h_data[i] = i;  // Initialize h_data

    cudaMalloc((void**)&d_data, size);  // Allocate device memory
    cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);  // Copy to device memory

    // Stream Creation Section
    cudaStream_t stream1, stream2;
    cudaStreamCreate(&stream1);
    cudaStreamCreate(&stream2);

    // Kernel Launch Section
    kernel1<<<N/256, 256, 0, stream1>>>(d_data);
    kernel2<<<N/256, 256, 0, stream2>>>(d_data);

    // Synchronization Section
    cudaStreamSynchronize(stream1);
    cudaStreamSynchronize(stream2);

    // Data Retrieval Section
    cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost);  // Copy results back to host

    // Output Section
    for (int i = 0; i < 10; i++) {
        std::cout << "h_data[" << i << "] = " << h_data[i] << std::endl;  // Print some results
    }

    // Cleanup Section
    cudaFree(d_data);  // Free device memory
    cudaFreeHost(h_data);  // Free pinned memory
    cudaStreamDestroy(stream1);  // Destroy stream1
    cudaStreamDestroy(stream2);  // Destroy stream2

    return 0;
}
```

**Why sync streams:** Synchronization ensures that the kernel executions and memory operations complete before proceeding with the next steps (like copying data back to the host or starting another kernel).

