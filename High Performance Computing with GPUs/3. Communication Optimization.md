## Planning Data Transfer

#### Minimize
Since, CPU <-->GPU memory transfer is expensive, we try to minimize necessary moves and try to eliminate unnecessary moves altogether. 

#### Frequency
Another thing we can do is transfer data less often i.e. Since the overhead of data transfer is persistent on both large chunks and small chunks, instead of sending small chunks many times, we try sending large chunks less often.

#### Pinned Memory
We can also use pinned memory. Pinned memory (**a.k.a Page-locked memory**) is memory in the RAM (host) that we explicitly reserve and lock in place. By doing this the GPU can access it directly without going through the OS's paging mechanism. This speeds up data transfer compared to <i >pageable</i> memory which is the default method of data transfer between host and device that maps virtual memory space to the physical memory pages in the RAM.
###### How to:
To do this in CUDA we use `cudaMallocHost((void**)A, size)` to pin a certain memory. After use we can use `cudaFreeHost(A)` to free that memory.

##### An example of pinned memory
```c
#include <cuda_runtime.h>
#include <iostream>

int main() {
    const int N = 5;
    int *h_A, *d_A; // Host and device pointers

    // Step 1: Allocate pinned memory on the host
    cudaMallocHost((void**)&h_A, N * sizeof(int));

    // Step 2: Fill the pinned memory with some values
    for (int i = 0; i < N; i++) {
        h_A[i] = i;
    }

    // Step 3: Allocate memory on the device
    cudaMalloc((void**)&d_A, N * sizeof(int));

    // Step 4: Copy data from pinned memory to device
    cudaMemcpy(d_A, h_A, N * sizeof(int), cudaMemcpyHostToDevice);

    // Step 5: Copy data back from device to pinned memory
    cudaMemcpy(h_A, d_A, N * sizeof(int), cudaMemcpyDeviceToHost);

    // Step 6: Print the data to verify the transfer
    for (int i = 0; i < N; i++) {
        std::cout << h_A[i] << " ";
    }
    std::cout << std::endl;

    // Step 7: Free the allocated memory
    cudaFree(d_A);
    cudaFreeHost(h_A);

    return 0;
}
```

## Streams 
In CUDA, a **stream** is a sequence of operations that the GPU executes in order. Operations in different streams can run at the same time, but operations in the same stream are done one after another. Tasks from different streams can be mixed together to make better use of the GPU.

###### How does this optimize memory and communication:
Streams allow for asynchronous execution, which means multiple tasks like memory transfer (e.g. host to device) can run at the same time as kernel executions. This improves GPU resource utilization i.e. the GPU doesn't have to sit idly while it waits for the data transfer.

![Screenshot 2025-04-08 225216](https://github.com/user-attachments/assets/09e1b438-0a51-4086-838b-8fc2cb8c9009)

To initiate streams in CUDA we use `cudaStreamCreate()` 

#### CUDA Memory Hierarchy Comparison

| Speed               | Memory Type     | Declaration Example   | Scope  |
| ------------------- | --------------- | --------------------- | ------ |
| Fastest             | Register        | `int v;` (implicit)   | thread |
| Very Fast           | Local Memory    | `int v[10];`          | thread |
| Fast                | Shared Memory   | `__shared__ int v;`   | block  |
| Fast (with caching) | Global Memory   | `__device__ int v;`   | grid   |
| Slowest             | Constant Memory | `__constant__ int v;` | grid   |
###### Key Points:
- **Registers** are the fastest but are limited in number, so excessive use can lead to spilling to local memory (which is slower).
- **Shared memory** is fast and shared between threads of the same block. Use when threads in the SM need to access the same data repeatedly. Faster than global and reduces the need to access global memory. Use in sorting algos, matmul, etc.
- **Constant memory** is cached and fast for read-only data, use especially when many threads need to read to the same data. Size is 64KB so pick constant data efficiently.

**Note: Read stencil computation code to understand how shared memory is used to reduce global memory access latency**


#### An example with Streams

```c
#include <cuda_runtime.h>
#include <iostream>

const int nstreams = 4; // Number of streams
const int eles_per_stream = 1000; // Elements per stream
int* h_A, * h_B, * h_C; // Host arrays
int* d_A, * d_B, * d_C; // Device arrays
cudaStream_t streams[nstreams]; // Array of streams

// Vector sum kernel to add two arrays element-wise
__global__ void vector_sum(int* A, int* B, int* C, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        C[idx] = A[idx] + B[idx];
    }
}

int main() {
    // Step 1: Allocate memory on host
    h_A = (int*)malloc(nstreams * eles_per_stream * sizeof(int));
    h_B = (int*)malloc(nstreams * eles_per_stream * sizeof(int));
    h_C = (int*)malloc(nstreams * eles_per_stream * sizeof(int));

    // Step 2: Allocate memory on device
    cudaMalloc((void**)&d_A, nstreams * eles_per_stream * sizeof(int));
    cudaMalloc((void**)&d_B, nstreams * eles_per_stream * sizeof(int));
    cudaMalloc((void**)&d_C, nstreams * eles_per_stream * sizeof(int));

    // Step 3: Initialize CUDA streams
    for (int i = 0; i < nstreams; i++) {
        cudaStreamCreate(&streams[i]);
    }

    // Step 4: Copy data to device and launch kernels
    for (int i = 0; i < nstreams; i++) {
        int offset = i * eles_per_stream;

        // Asynchronous copy of data from host to device
        cudaMemcpyAsync(&d_A[offset], &h_A[offset], eles_per_stream * sizeof(int), cudaMemcpyHostToDevice, streams[i]);
        cudaMemcpyAsync(&d_B[offset], &h_B[offset], eles_per_stream * sizeof(int), cudaMemcpyHostToDevice, streams[i]);

        // Launch vector sum kernel asynchronously on stream[i]
        vector_sum<<<(eles_per_stream + 255) / 256, 256, 0, streams[i]>>>(d_A + offset, d_B + offset, d_C + offset, eles_per_stream);

        // Asynchronous copy from device to host
        cudaMemcpyAsync(&h_C[offset], &d_C[offset], eles_per_stream * sizeof(int), cudaMemcpyDeviceToHost, streams[i]);
    }

    // Step 5: Synchronize streams to ensure all operations are completed
    for (int i = 0; i < nstreams; i++) {
        cudaStreamSynchronize(streams[i]);
    }

    // Step 6: Free memory
    for (int i = 0; i < nstreams; i++) {
        cudaStreamDestroy(streams[i]);
    }

    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
    free(h_A);
    free(h_B);
    free(h_C);

    return 0;
}

```

**Why sync streams:** Synchronization ensures that the kernel executions and memory operations complete before proceeding with the next steps (like copying data back to the host or starting another kernel).

