CPU and GPU have separate memory (RAM and VRAM respectively). Because of this we manually have to malloc and memcpy. This causes issues like writing a lot of repetitive boilerplate code, extra memory copies made, time-taking, hard to constantly manage memory at such a low level.

## Unified Memory Model
#### What is it?

- **One memory space** for both CPU and GPU.
    
- GPU and CPU can access the **same data**, same address.
    
- No more `cudaMemcpy()`. Just `cudaMallocManaged()`
#### How it works

| Concept                              | Description                                                                                      |
| ------------------------------------ | ------------------------------------------------------------------------------------------------ |
| **Unified Virtual Addressing (UVA)** | Creates a **single virtual address space** shared by CPU and GPU.                                |
| **On-Demand Page Migration**         | Memory pages are **moved automatically** to the device (CPU or GPU) **when accessed**.           |
| **Lazy Allocation**                  | Memory is **not physically allocated** at `cudaMallocManaged()` — it's done **on first access**. |

1) When CPU or GPU accesses a `data[i]`, CUDA checks if it's **available locally**.
2) If not, it **migrates the memory page** to the current processor — either CPU RAM or GPU VRAM.
3) It tracks this using **page faults** and **UVA** (Unified Virtual Addressing).

#### Is There Hardware Involved?
Yes — but:
- Pre-Pascal GPUs (before 2016) handled Unified Memory **mostly in software**.
    
- Pascal (GTX 10xx) and newer GPUs have **hardware support** to make migration **faster and smarter**.

#### Use-case

**DL & AI:** Handles large datasets (larger than the GPU's VRAM)
**Scientific Computing:** Allows massive simulations to run across CPU & GPU.
**Big Data Analytics:** Reduces memory management complexity in GPU databases.
**HPC & Supercomputing:** Enables efficient memory sharing across heterogeneous
systems.

#### Example Code
```c
int *data;

// Allocate Unified Memory (accessible by both CPU & GPU)
cudaMallocManaged(&data, size);

// GPU uses the data
kernel<<<blocks, threads>>>(data);
cudaDeviceSynchronize();  // wait till GPU finishes

// Free memory
cudaFree(data);
```


#### Benefits

|Feature|What it Means|Why it’s Useful|
|---|---|---|
|**Page Migration Engine**|Automatically moves memory pages between CPU ↔ GPU when accessed.|You don't have to manually manage memory — easier coding.|
|**Oversubscription**|You can allocate more than GPU memory — it spills over to CPU RAM.|Enables **out-of-core** computation (handling data larger than VRAM).|
|**System-wide Atomics**|Atomics can work across CPU and GPU memory.|Better for coordination in shared memory spaces.|
|**Good for Sparse Access**|Page faults load only needed pages.|Efficient for applications that don’t access all memory uniformly.|

#### Issues
| Issue                           | Why It Hurts                                                                          |
| ------------------------------- | ------------------------------------------------------------------------------------- |
| **Page Faults = Slow**          | Each page fault costs time. If you rely on faults to move lots of data, you're toast. |
| **Not Good for Bulk Transfers** | Better to prefetch or use cudaMemcpy-style bulk transfers.                            |
| **Initial Access Delay**        | First access causes migration = overhead.                                             |
#### Fix: Prefetching

- Use `cudaMemPrefetchAsync()` to move data **before** the kernel runs.
    
- Prevents runtime page faults and saves performance.

###### Example
```c
int N = 1 << 20; // 1 million floats
float *x;

// Allocate unified memory (accessible by both CPU and GPU)
cudaMallocManaged(&x, N * sizeof(float));

// Optional: initialize x on CPU
for (int i = 0; i < N; i++) {
    x[i] = i;
}

// Prefetch x to the current GPU (so it doesn’t page-fault later)
int device = -1;
cudaGetDevice(&device);
cudaMemPrefetchAsync(x, N * sizeof(float), device);

```


## **Bank Conflicts**:

- **What is a Bank Conflict?**
    
    - A bank conflict occurs when **multiple threads** within the same warp attempt to access **memory locations** that belong to the **same memory bank** simultaneously.
        
    - This leads to **serialization**, where only one thread’s memory access can happen at a time, thus reducing memory bandwidth.
        
- **Impact of Bank Conflicts:**
    
    - If there are no bank conflicts, each bank can return data to the threads **without delays**.
        
    - However, if there are conflicts, the latency increases since the conflicting accesses have to be serialized (one by one).

### **No Conflict (Broadcast) Solution**:

- If **all threads** in the warp access the **same address**, the bank can **broadcast** the data to all threads simultaneously, avoiding any delay or conflict.

#### Code Example
```c
__global__ void broadcastExample() {
    // Assume shared_data is a shared memory array
    __shared__ int shared_data[32];
    
    // Thread 0 loads the value
    if (threadIdx.x == 0) {
        shared_data[0] = 42; // Set value at index 0
    }
    
    // All threads in the warp access the same memory location
    __syncthreads();  // Synchronize to make sure the value is written
    
    // All threads read the same value
    int value = shared_data[0]; // This is a broadcast read
    
    printf("Thread %d received value %d\n", threadIdx.x, value);
}
```