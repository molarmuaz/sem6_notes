In CUDA we use Atomics to ensure that an operation is performed on a memory location by one thread at a time, and does not allow any other threads to interfere with that memory and that operation during this time.

This also ensures that the memory is updated correctly when multiple threads try to read or write to the same memory location concurrently (Critical Section). 

They prevent race conditions and ensure the integrity of the data.

 When one thread performs an atomic operation on a memory location, the operation locks that memory location to avoid any interference from other threads.
 
#### Common Atomic Operations in CUDA:

- `int atomicAdd(int* address, int value)`:  Adds a value to a memory location.  
	 
- `int atomicSub(int* address, int value)` :  Subtracts a value from a memory location.  
	
- `int atomicExch(int* address, int value)`: Exchanges the value of a memory location.  
	
- `int atomicCAS(int* address, int compare, int value)`:  Performs a conditional swap based on a comparison.  
	
- `int atomicMin(int* address, int value)` :  Sets the memory location to the minimum of its current value and the given value.  
	
- `int atomicMax(int* address, int value)`:  Sets the memory location to the maximum of its current value and the given value.  
	
- `int atomicAnd(int* address, int value)`:  Performs a bitwise AND operation atomically.  
	
- `int atomicOr(int* address, int value)` :  Performs a bitwise OR operation atomically.  
	
- `int atomicXor(int* address, int value)`:  Performs a bitwise XOR operation atomically.  
	
- `unsigned int atomicInc(unsigned int* address, unsigned int value)`: Increments the value at the memory location by 1  unless greater than value then it goes back to 0.  
	  
- `unsigned int atomicDec(unsigned int* address, unsigned int value)`:  Decrements the value at the memory location by 1 unless less than value.  

#### How they work
Atomic operations are performed by a single instruction set architecture (ISA) command on a memory location which means the function calls shown above are translated into intrinsics (single instructions), where the thread reads the current value, computes a new value, and writes into the locked memory back. 

The hardware ensures that no other threads can access or modify the memory location until the atomic operation is completed, effectively serializing access to that location. Threads that attempt to perform the operation on the same memory location are queued, with each thread executing the operation one after the other, preventing race conditions and ensuring data consistency.

## Latency Problem
Each atomic operation involves reading a value from memory, modifying it, and then writing it back. This process introduces significant latency because each operation requires a read and a write cycle, each with a delay of several hundred cycles.

Throughput (rate at which atomic operation can be performed) drops significantly when a lot of threads try to access the same memory location. This creates a memory bandwidth bottleneck, and so the throughput is a fraction of what it should be.

#### Improvements
Two suggested improvements are, 
- Carry out atomic operations on a shared memory, while still serialized, faster memory fetching and since shared memory is private to blocks, globally the contention is not as big of an issue, however it is trickier to program.
	
- if in a Fermi architecture execute the atomic operations on an F2 cache, while still serialized it can be a bit faster. 

###### Atomic Operations in Shared Memory
In order to carry out atomic operations in shared memory instead of global memory we privatize data.

**Privatization** involves creating a **private copy** of a shared data structure for each thread block to reduce contention. This is done by utilizing **shared memory** for temporary data storage, so threads within a block do not need to access global memory.

###### Example
```c
#include <stdio.h>
#include <cuda_runtime.h>

__global__ void kernel(int *globalData) {
    __shared__ int shared[256];

    // Copy from global to shared (1:1 mapping for this example)
    if (threadIdx.x < 256)
        shared[threadIdx.x] = globalData[threadIdx.x];

    __syncthreads(); // Make sure everyone copied before continuing

    // Atomic add in shared memory
    if (threadIdx.x < 256)
        atomicAdd(&shared[0], 1); // Everyone adds to shared[0]

    __syncthreads(); // Wait for all threads to finish atomic ops

    // Write back to global memory
    if (threadIdx.x == 0)
        globalData[0] = shared[0];
}

int main() {
    int h_data[256] = {0};
    int *d_data;
    cudaMalloc(&d_data, sizeof(h_data));
    cudaMemcpy(d_data, h_data, sizeof(h_data), cudaMemcpyHostToDevice);

    kernel<<<1, 256>>>(d_data);
    cudaMemcpy(h_data, d_data, sizeof(h_data), cudaMemcpyDeviceToHost);

    printf("Final value at globalData[0] = %d\n", h_data[0]);

    cudaFree(d_data);
    return 0;
}
```

Since shared memory per block is limited in size (usually 48KB), if the global memory is too big to convert to shared, we can maybe break it down into chunks and regather them after the operations are done.


<center>note: always synchronize your threads and device and streams after operating </center>


