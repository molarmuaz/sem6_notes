# 2. OpenCL
## Platform Model

The OpenCL platform model defines the fundamental structure of an OpenCL system:

### Components:

1. **Host**:
    - Central control unit (typically a CPU)
    - Manages execution environment and dispatches commands
2. **OpenCL Devices**:
    - Computational units where parallel processing occurs
    - Contains multiple Compute Units (CUs)
3. **Compute Units (CUs)**:
    - Subdivisions of OpenCL devices
    - Contains multiple Processing Elements (PEs)
4. **Processing Elements (PEs)**:
    - Smallest computational units
    - Execute individual work-items
### Define Host

Basically: We have a host and devices. The host sets up the functions, the devices, contexts and queues.

The basic 5 steps to making a host program
- **Define** the platform (devices, contexts and queues)
- **Create and Build** the program (dynamic library for kernels)
- **Setup** memory objects
- **Define** the kernel (attach arguments to kernel function)
- **Submit** commands (transfer memory objects and execute kernels)
```c
// Get platform and device
cl::Platform platform;
cl::Device device;  
cl::Context context;
cl::CommandQueue queue;
```

**Platform:** Available Platforms like NVIDIA, AMD, etc

**Device:** Available Devices from those platforms e.g NVIDIA GeForce RTX 3080 GPU

**Context:** An environment/sandbox for devices to interact, manage memory, loading and executing     kernel functions, etc

**Command Queue:** Queue of commands for a device, includes; kernel executions, memory transfers  and syncronizations.


#### What it looks like in code
```cpp
std::vector<cl::Platform> platforms;
cl::Platform::get(&platforms); //Get platforms list
cl::Platform platform = platforms[0]; //Pick the first platform

std::vector<cl::Device> devices; //Creat device list
platform.getDevices(CL_DEVICE_TYPE_GPU, &devices); // Fill with platform's devices
cl::Device device = devices[0]; //Pick the first device

cl::Context context(device); // setup environment
cl::CommandQueue queue(context, device); // queue to send commands to GPU
```

#### A quicker way to setup (less safe and we don't pick the device)
```cpp
cl::Context context(CL_DEVICE_TYPE_DEFAULT);
/*
Also valid:
CL_DEVICE_TYPE_CPU,
CL_DEVICE_TYPE_GPU,
CL_DEVICE_TYPE_ACCELERATOR
*/

//It'll pick the first platform returned from getPlatforms, and add all the //devices from that into the context based on what device type we choose.
```

#### Queue Types:
- Each command queue operates on one device within a context only.
- We can configure command queues in different ways to organize which command gets executed first:
	- In order queue: Completed in order they appear in host program
	- Out of order queue: Completed in an order of its own choice.
		- **Why out of order:** Out-of-order queues let the GPU **optimize execution** by reordering commands if it can do so **more efficiently**, as long as data dependencies are preserved
##### How to setup out of order queueing:
```cpp
cl_command_queue_properties props = CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE;

// Create queue
cl::CommandQueue queue(context, device, props);
```

#### Building the kernel program
When defining the source code of the kernel program, we can either define it within the program with a string literal (if it is a small program/instruction), or create a separate`.cl` file to write the function and then call it in the main program as a string. 

We package this kernel as a `cl::Program` object. It gets compiled at runtime for the target device. This object is used to make the kernel (`cl::Kernel`), set arguments and then it gets enqueued on the command queue.
**Simply put:**
[kernel.cl] → [read as string] → [cl::Program] → [cl::Kernel] → [enqueue for execution]

#### In-code (Vector Addition Example)
```cpp
#include <iostream>
#include <vector>
#include <CL/cl.hpp>
int main() {
	std::vector<float> a = {1.0f, 2.0f, 3.0f, 4.0f};
	std::vector<float> b = {4.0f, 3.0f, 2.0f, 1.0f};
	std::vector<float> c(a.size());
	try {
	// get available OpenCL platforms
	std::vector<cl::Platform> platforms;
	cl::Platform::get(&platforms);

	// choose a platform
	cl::Platform platform = platforms[0];
	
	// get available OpenCL devices
	std::vector<cl::Device> devices;
	platform.getDevices(CL_DEVICE_TYPE_ALL, &devices);
	
	// choose a device
	cl::Device device = devices[0];
	
	// create an OpenCL context for the device
	cl::Context context({device});
	
	// create an OpenCL program from source
	cl::Program program(context, "kernel.cl");
	
	// build the program for the device
	program.build({device});
	
	// create OpenCL buffers for the input and output vectors
	cl::Buffer bufA(context, CL_MEM_READ_ONLY, a.size() * sizeof(float));
	cl::Buffer bufB(context, CL_MEM_READ_ONLY, b.size() * sizeof(float));
	cl::Buffer bufC(context, CL_MEM_WRITE_ONLY, c.size() * sizeof(float));

	// create a command queue for the device
	cl::CommandQueue queue(context, device);

	// enqueue data to be transferred to the device
	queue.enqueueWriteBuffer(bufA, CL_TRUE, 0, a.size() * sizeof(float), a.data());
	queue.enqueueWriteBuffer(bufB, CL_TRUE, 0, b.size() * sizeof(float), b.data());

	// create a kernel object for the vector addition kernel
	cl::Kernel kernel(program, "vecadd");
	
	// set the arguments of the kernel
	kernel.setArg(0, bufA);
	kernel.setArg(1, bufB);
	kernel.setArg(2, bufC);

	// enqueue the kernel for execution
	queue.enqueueNDRangeKernel(kernel, cl::NullRange, cl::NDRange(a.size()), cl::NullRange);

	// enqueue data to be transferred back to the host
	queue.enqueueReadBuffer(bufC, CL_TRUE, 0, c.size() * sizeof(float), c.data());

	// print the result
	for (float x : c) {
		std::cout << x << " ";
	}
	std::cout << std::endl;
	} 
	catch (cl::Error& e) {
	std::cerr << "OpenCL error: " << e.what() << " (" << e.err() << ")" << std::endl;

	return 1;
	}
	return 0;
}
```

#### Scope
- Global Dimensions = size of the whole problem.
    
- Local Dimensions = size of a chunk processed by a group of workers.
    
- Choose 1D, 2D, or 3D depending on your data shape and algorithm needs. For your 2D problem, 2D NDRange is the natural fit.

### Kernel Programming

We use `__kernel` to use  define a kernel function.

| Qualifier    | Meaning                                             |
| ------------ | --------------------------------------------------- |
| `__global`   | Pointer refers to global memory (e.g., on GPU)      |
| `__local`    | Pointer refers to shared memory within a work-group |
| `__private`  | Each work-item’s own local variable                 |
| `__constant` | Read-only global memory on a GPU                    |

#### Work item functions
| **Function**         | **Returns** | **Description**                                                  | Example 2D Range |
| -------------------- | ----------- | ---------------------------------------------------------------- | ---------------- |
| `get_work_dim()`     | `uint`      | Number of dimensions used (1, 2, or 3)                           | `2`              |
| `get_global_id(n)`   | `size_t`    | Global ID of the work-item in dimension `n` (absolute index)     | `512`            |
| `get_local_id(n)`    | `size_t`    | Local ID of the work-item within its work-group in dimension `n` | `16`             |
| `get_group_id(n)`    | `size_t`    | ID of the work-group in dimension `n`                            | `4`              |
| `get_global_size(n)` | `size_t`    | Total number of global work-items in dimension `n`               | `1024`           |
| `get_local_size(n)`  | `size_t`    | Number of work-items in a work-group in dimension `n`            | `64`             |

#### Big Idea
Essentially, we want to avoid using serial loops. OpenCL directly helps with that by parallelizing the problem instead, so that multiple iterations can be completed simultaneously by different work items.

###### Traditional Loop
```cpp
void mul(const int n, const float *a, const float *b, float *c)
{
	for (int i = 0; i < n; i++)
	{
		c[i] = a[i] * b[i];
	}
}
```
###### OpenCL
```cpp
__kernel void mul( __global const float *a, __global const float *b, __global float *c)
{
	int id = get_global_id(0);
	c[id] = a[id] * b[id];
}

//divided over n work items
```

#### Barriers
Used to **synchronize all work-items** within a work-group.

**Use case**: You want all work-items to finish writing to local memory before continuing.
```cpp
__local int shared[256];

int lid = get_local_id(0);
shared[lid] = lid;

barrier(CLK_LOCAL_MEM_FENCE); // Wait until all writes are done

int val = shared[(lid + 1) % 256]; // Safe read
```

#### Memory Fences
Ensure memory operations happen in order (more fine-grained than a barrier).

Use when you’re managing memory order manually between global/local reads/writes.
```cpp 
mem_fence(CLK_GLOBAL_MEM_FENCE); 
```


#### Matrix Multiplication Example
```cpp
__kernel void matmul(
    __global float* A, // MxK
    __global float* B, // KxN
    __global float* C, // MxN
    
    const int M,
    const int N,
    const int K
) {
    int row = get_global_id(1); // Y-dimension: row of C
    int col = get_global_id(0); // X-dimension: col of C

    if (row < M && col < N) {
        float sum = 0.0f;

        for (int i = 0; i < K; i++) {
            sum += A[row * K + i] * B[i * N + col];
        }

        C[row * N + col] = sum;
    }
}
```

#### Host End

```cpp
#define DEVICE CL_DEVICE_TYPE_DEFAULT

// declarations (not shown)
sz = N * N;
std::vector<float> h_A(sz);
std::vector<float> h_B(sz);
std::vector<float> h_C(sz);
cl::Buffer d_A, d_B, d_C;
// initialize matrices and setup
// the problem (not shown)

cl::Context context(DEVICE);
cl::Program program(context, util::loadProgram("matmul1.cl"), true);

cl::CommandQueue queue(context);
cl::make_kernel<int, cl::Buffer, cl::Buffer, cl::Buffer>mmul(program, "mmul");

d_A = cl::Buffer(context, h_A.begin(), h_A.end(), true);

d_B = cl::Buffer(context, h_B.begin(), h_B.end(), true);

d_C = cl::Buffer(context, CL_MEM_WRITE_ONLY, sizeof(float) * sz);

mmul(cl::EnqueueArgs(queue, cl::NDRange(N,N)),N, d_A, d_B, d_C );
cl::copy(queue, d_C, h_C.begin(), h_C.end());
```

### Memory Heirarchy
|Memory Type|Scope|Typical Size|Bandwidth (Speed)|Accessed By|Latency|
|---|---|---|---|---|---|
|**Private**|Per work-item|~10s of words/WI|~2–3 words/cycle/WI|One work-item|Very low|
|**Local**|Per work-group|~1–10 KB/WG|~10s of words/cycle/WG|All WIs in a WG|Low|
|**Global**|All work-groups|~1–10 GB|~100–200 GB/s (shared bus)|All work-items|High|
|**Constant**|Read-only global scope|Shared with Global|Cached, slightly faster|All work-items|Medium|
|**Host**|CPU memory|~1–100 GB|~1–50 GB/s via PCIe (slow)|CPU + transfers|Very high|