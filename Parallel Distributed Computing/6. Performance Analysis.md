# 6. Performance Analysis
One of the ways we can measure the performance of Programs or Computer systems is through **Wall Clock Time** i.e. the _real-world time_ it takes to complete a task. 

The execution time of **Program A** can be split into:
- **User CPU Time** – time spent by your program.
- **System CPU Time** – time spent by OS functions called by your program.
- **Waiting Time** – time lost to I/O or multitasking delays.
Focus here is on **user CPU time**, because it's what we can optimize most directly.

##### We can measure system performance with the following metrics:
- **Clock Speed:** How many cycles can be completed in one second
- **MIPS:** How many (in Millions) instructions are are executed per second
- **FLOPS (Best):** How many floating point operations are executed per second. Used usually in numerical tasks (scientific work, data/AI related work, etc)
- **Benchmark Tests:** Standardized tests used to evaluate and **compare performance** across systems.

##### Factors affecting Computer Performance
- **Processor Speed**
- **Data Bus width**
- **Amount of cache**
- **Faster interfaces**
- **Amount of main memory**

## Performance Metrics – Parallel Systems

### Amdahl's Law & Speedup Factor

Amdahl's Law states that potential program speedup is defined by the fraction of code (P) that can be parallelized:
$$
Speedup = \frac{1}{(1 - P) + \frac{P}{S}}
$$

- P = parallelizable portion
- S = speedup of the parallel portion



![[file-.jpg]]

While max possible speedup with p processors is p (linear) , it is possible to get super-linear speedup (greater than p) usually due to:
- **Extra memory** in multiprocessor system
- **Nondeterministic Algorithm**

However in practice even linear speedup is rarely achieved, especially moving towards larger number of processors.

#### Efficiency
The ability to avoid wasting materials, energy, efforts, money, and time in doing something or in producing a desired result

$$ Efficiency = \frac{Speedup}{Number of Processors}$$

Basically asking, are we fully utilizing the processors we added?

![[file- 1.jpg]]

###### Explanation
As you can see, moving from left to right, efficiency decreases. This is also evident from the decreasing ratio of speedup between the processor size i and i+1. This means the new processors are speeding up our process but are not getting utilized to the fullest. 

We can see this is improved when the problem size is increased. Increasing the problem size results in more utilization.

Based on our requirements, sometimes the speedup is not worth increasing processors, and this we can judge with efficiency

### Gustafson's Law

Amdahl's Law works for a fixed size problem, however as outlined above, we see that the same number of processors offer different speedups based on the problem size. 

Gustafson's Law argues that efficiency of the number of processors can be retained by the increase of the problem size. 

#### Time constrained scaling
The main difference shows itself here. While Amdahl's law expects the time to reduce with the increase of  processors, we expect the time to remain constant as the problem increases. 

For example, we want to measure how efficiently we can render the game with an even higher resolution (bigger problem size) within the same amount of time. For this estimation we will use Gustafson's Law.

It is important to note that this is all in consideration that parallelizable part will be increased and not serial. The increase of non-parallelizable part will surely result in an increase of time.

#### Formula
$$Scaled-Speedup = p+(1-p)s$$
**p** -> Number of processors
**s** -> Non-parallelizable portion

#### Gustafson’s Law (Summary):  
Unlike Amdahl’s Law, which assumes a fixed problem size, Gustafson’s Law fixes the execution time and allows the problem size to scale with more processors. It’s more practical in real-world scenarios where more computing power is used to solve bigger problems, not just finish small ones faster.

##### Scalability Types:

- **Strong Scalability:** Efficiency remains constant as more processors are added **without changing** problem size.
    
- **Weak Scalability:** Efficiency remains constant as both the **problem size and processors** increase proportionally.






